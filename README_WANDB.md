# Wandbé›†æˆä½¿ç”¨æŒ‡å—

## ğŸ“Š æ¦‚è¿°

æœ¬é¡¹ç›®å·²é›†æˆWandbï¼ˆWeights & Biasesï¼‰å®éªŒè·Ÿè¸ªåŠŸèƒ½ï¼Œå¯ä»¥å®æ—¶ç›‘æ§è®­ç»ƒå’Œè¯„ä¼°çš„losså’Œå›°æƒ‘åº¦ï¼ˆPPLï¼‰ã€‚

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. å®‰è£…wandb
```bash
pip install wandb
```

### 2. ç™»å½•wandb
```bash
wandb login
```
é¦–æ¬¡ä½¿ç”¨éœ€è¦åœ¨ https://wandb.ai æ³¨å†Œè´¦å·å¹¶è·å–API keyã€‚

### 3. å¯ç”¨wandbè®­ç»ƒ
```bash
# DeepSpeedè®­ç»ƒï¼ˆé»˜è®¤å¯ç”¨wandbï¼‰
./scripts/run_pretrain_deepspeed.sh

# ç®€å•è®­ç»ƒï¼ˆé»˜è®¤å¯ç”¨wandbï¼‰
./scripts/run_pretrain_simple.sh

# æ‰‹åŠ¨æŒ‡å®šwandbå‚æ•°
python train/pretrain.py \
    --use_wandb \
    --wandb_project "my-gpt-project" \
    --wandb_run_name "experiment-1" \
    --output_dir ./outputs \
    --batch_size 8 \
    --learning_rate 1e-4 \
    --num_epochs 10
```

## ğŸ“ˆ ç›‘æ§æŒ‡æ ‡

### è®­ç»ƒæŒ‡æ ‡ (train/)
- **train/loss**: è®­ç»ƒæŸå¤±
- **train/perplexity**: è®­ç»ƒå›°æƒ‘åº¦
- **train/learning_rate**: å­¦ä¹ ç‡
- **train/epoch**: å½“å‰è½®æ¬¡
- **train/step**: å…¨å±€æ­¥æ•°
- **train/elapsed_time**: æ¯æ­¥ç”¨æ—¶

### è¯„ä¼°æŒ‡æ ‡ (eval/)
- **eval/loss**: éªŒè¯é›†æŸå¤±
- **eval/perplexity**: éªŒè¯é›†å›°æƒ‘åº¦
- **eval/step**: è¯„ä¼°æ­¥æ•°
- **eval/best_perplexity**: å†å²æœ€ä½³å›°æƒ‘åº¦

### è½®æ¬¡æŒ‡æ ‡ (epoch/)
- **epoch/train_loss**: è½®æ¬¡å¹³å‡è®­ç»ƒæŸå¤±
- **epoch/train_perplexity**: è½®æ¬¡å¹³å‡è®­ç»ƒå›°æƒ‘åº¦
- **epoch/val_loss**: è½®æ¬¡éªŒè¯æŸå¤±
- **epoch/val_perplexity**: è½®æ¬¡éªŒè¯å›°æƒ‘åº¦
- **epoch/number**: è½®æ¬¡ç¼–å·
- **epoch/best_perplexity**: å½“å‰æœ€ä½³å›°æƒ‘åº¦

### æœ€ç»ˆæŒ‡æ ‡ (final/)
- **final/best_perplexity**: æœ€ç»ˆæœ€ä½³å›°æƒ‘åº¦
- **final/total_steps**: æ€»è®­ç»ƒæ­¥æ•°
- **final/total_epochs**: æ€»è®­ç»ƒè½®æ•°

## ğŸ¯ ä½¿ç”¨ç¤ºä¾‹

### åŸºæœ¬ä½¿ç”¨
```bash
# å¯ç”¨wandbçš„ç®€å•è®­ç»ƒ
./scripts/run_pretrain_simple.sh
```

### è‡ªå®šä¹‰é¡¹ç›®åç§°
ä¿®æ”¹è„šæœ¬ä¸­çš„wandbå‚æ•°ï¼š
```bash
# åœ¨è„šæœ¬ä¸­ä¿®æ”¹
WANDB_PROJECT="my-custom-project"
WANDB_RUN_NAME="experiment-$(date +%Y%m%d_%H%M%S)"
```

### ç¦ç”¨wandb
```bash
# ä¿®æ”¹è„šæœ¬ä¸­çš„USE_WANDB=false
# æˆ–è€…ç›´æ¥è¿è¡Œpythonå‘½ä»¤ä¸åŠ --use_wandbå‚æ•°
python train/pretrain.py --output_dir ./outputs --batch_size 8 --num_epochs 3
```

## ğŸ“Š Wandbç•Œé¢åŠŸèƒ½

### 1. å®æ—¶å›¾è¡¨
- **Lossæ›²çº¿**: è®­ç»ƒå’ŒéªŒè¯æŸå¤±çš„å®æ—¶å˜åŒ–
- **å›°æƒ‘åº¦æ›²çº¿**: è®­ç»ƒå’ŒéªŒè¯å›°æƒ‘åº¦çš„å®æ—¶å˜åŒ–
- **å­¦ä¹ ç‡æ›²çº¿**: å­¦ä¹ ç‡è°ƒåº¦çš„å˜åŒ–

### 2. ç³»ç»Ÿç›‘æ§
- **GPUä½¿ç”¨ç‡**: å®æ—¶GPUåˆ©ç”¨ç‡å’Œæ˜¾å­˜ä½¿ç”¨
- **CPUä½¿ç”¨ç‡**: CPUå’Œå†…å­˜ä½¿ç”¨æƒ…å†µ
- **ç½‘ç»œI/O**: æ•°æ®åŠ è½½æ€§èƒ½

### 3. è¶…å‚æ•°è·Ÿè¸ª
- **æ¨¡å‹é…ç½®**: å±‚æ•°ã€éšè—ç»´åº¦ã€æ³¨æ„åŠ›å¤´æ•°ç­‰
- **è®­ç»ƒé…ç½®**: æ‰¹é‡å¤§å°ã€å­¦ä¹ ç‡ã€ä¼˜åŒ–å™¨å‚æ•°ç­‰
- **æ•°æ®é…ç½®**: åºåˆ—é•¿åº¦ã€è¯è¡¨å¤§å°ç­‰

### 4. å®éªŒæ¯”è¾ƒ
- **å¤šå®éªŒå¯¹æ¯”**: ä¸åŒè¶…å‚æ•°è®¾ç½®çš„æ•ˆæœå¯¹æ¯”
- **æœ€ä½³æ¨¡å‹è¿½è¸ª**: è‡ªåŠ¨è®°å½•æœ€ä½³å›°æƒ‘åº¦å’Œå¯¹åº”çš„æ¨¡å‹

## ğŸ”§ é…ç½®é€‰é¡¹

### å‘½ä»¤è¡Œå‚æ•°
```bash
--use_wandb              # å¯ç”¨wandbè·Ÿè¸ª
--wandb_project PROJECT  # wandbé¡¹ç›®åç§°ï¼ˆé»˜è®¤ï¼šgpt-pretrainï¼‰
--wandb_entity ENTITY    # wandbå›¢é˜Ÿåç§°ï¼ˆå¯é€‰ï¼‰
--wandb_run_name NAME    # è¿è¡Œåç§°ï¼ˆå¯é€‰ï¼Œé»˜è®¤è‡ªåŠ¨ç”Ÿæˆï¼‰
```

### é…ç½®ç±»å‚æ•°
åœ¨ `PretrainConfig` ç±»ä¸­ï¼š
```python
self.use_wandb = True                    # æ˜¯å¦ä½¿ç”¨wandb
self.wandb_project = "gpt-pretrain"      # é¡¹ç›®åç§°
self.wandb_entity = None                 # å›¢é˜Ÿåç§°
self.wandb_run_name = None               # è¿è¡Œåç§°
```

## ğŸ“ æœ€ä½³å®è·µ

### 1. å‘½åè§„èŒƒ
```bash
# æ¨èçš„è¿è¡Œåç§°æ ¼å¼
WANDB_RUN_NAME="model-${MODEL_SIZE}-lr-${LEARNING_RATE}-$(date +%Y%m%d_%H%M%S)"

# ä¾‹å¦‚
WANDB_RUN_NAME="model-6layer-lr-1e4-20241201_120000"
```

### 2. é¡¹ç›®ç»„ç»‡
```bash
# æŒ‰å®éªŒç±»å‹ç»„ç»‡é¡¹ç›®
WANDB_PROJECT="gpt-pretrain-baseline"     # åŸºçº¿å®éªŒ
WANDB_PROJECT="gpt-pretrain-ablation"     # æ¶ˆèå®éªŒ
WANDB_PROJECT="gpt-pretrain-hyperopt"     # è¶…å‚æ•°ä¼˜åŒ–
```

### 3. æ ‡ç­¾ä½¿ç”¨
åœ¨wandbç•Œé¢ä¸­ä¸ºå®éªŒæ·»åŠ æ ‡ç­¾ï¼š
- `baseline`: åŸºçº¿æ¨¡å‹
- `large-model`: å¤§æ¨¡å‹å®éªŒ
- `fast-training`: å¿«é€Ÿè®­ç»ƒæµ‹è¯•
- `production`: ç”Ÿäº§ç¯å¢ƒæ¨¡å‹

## ğŸ” æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

1. **wandbæœªå®‰è£…**
   ```bash
   pip install wandb
   ```

2. **æœªç™»å½•wandb**
   ```bash
   wandb login
   ```

3. **ç½‘ç»œè¿æ¥é—®é¢˜**
   ```bash
   # è®¾ç½®ä»£ç†ï¼ˆå¦‚æœéœ€è¦ï¼‰
   export WANDB_BASE_URL="https://api.wandb.ai"
   ```

4. **ç¦ç”¨wandb**
   ```bash
   # åœ¨è„šæœ¬ä¸­è®¾ç½®
   USE_WANDB=false
   
   # æˆ–è€…ä¸ä½¿ç”¨--use_wandbå‚æ•°
   ```

### ç¦»çº¿æ¨¡å¼
å¦‚æœç½‘ç»œä¸ç¨³å®šï¼Œå¯ä»¥ä½¿ç”¨ç¦»çº¿æ¨¡å¼ï¼š
```bash
export WANDB_MODE=offline
```

ç¨ååŒæ­¥ï¼š
```bash
wandb sync wandb/offline-run-*
```

## ğŸ“Š ç¤ºä¾‹è¾“å‡º

è®­ç»ƒå¼€å§‹æ—¶ä¼šçœ‹åˆ°ï¼š
```
wandbåˆå§‹åŒ–æˆåŠŸ
wandb: Currently logged in as: your-username
wandb: Tracking run at https://wandb.ai/your-username/gpt-pretrain/runs/abc123
```

è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ—¥å¿—ï¼š
```
æ­¥éª¤    250 | è½®æ¬¡  1/10 | æ‰¹æ¬¡  125/500 | æŸå¤± 4.2345 | å›°æƒ‘åº¦  68.45 | å­¦ä¹ ç‡ 8.5e-05 | ç”¨æ—¶ 1.23s
--- ç¬¬ 500 æ­¥è¯„ä¼° ---
è¯„ä¼°å®Œæˆ - å¹³å‡æŸå¤±: 3.8934, å›°æƒ‘åº¦: 49.12
```

å¯¹åº”çš„wandbæŒ‡æ ‡ä¼šå®æ—¶æ›´æ–°åˆ°ç½‘é¡µç•Œé¢ã€‚

## ğŸ‰ æ€»ç»“

é€šè¿‡wandbé›†æˆï¼Œæ‚¨å¯ä»¥ï¼š
- å®æ—¶ç›‘æ§è®­ç»ƒè¿›åº¦
- æ¯”è¾ƒä¸åŒå®éªŒçš„æ•ˆæœ
- è¿½è¸ªæœ€ä½³æ¨¡å‹æ€§èƒ½
- åˆ†æè®­ç»ƒè¿‡ç¨‹ä¸­çš„é—®é¢˜
- ä¸å›¢é˜Ÿåˆ†äº«å®éªŒç»“æœ

ç°åœ¨å¼€å§‹æ‚¨çš„wandbå¢å¼ºè®­ç»ƒå§ï¼

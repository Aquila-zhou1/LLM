"""
æ¨¡å‹è¯„ä¼°è„šæœ¬
ç”¨äºè¯„ä¼°è®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œè®¡ç®—å›°æƒ‘åº¦å’Œç”Ÿæˆæ–‡æœ¬ç¤ºä¾‹
"""

import os
import sys
import json
import argparse
from pathlib import Path

import torch
import torch.nn as nn
from transformers import GPT2Tokenizer

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(str(Path(__file__).parent.parent))

from data.tinystories_loader import create_dataloaders
from model.gpt_model import GPTSmall

import logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def load_model_from_checkpoint(checkpoint_path: str, device: torch.device) -> tuple[GPTSmall, dict]:
    """
    ä»æ£€æŸ¥ç‚¹åŠ è½½æ¨¡å‹
    
    Args:
        checkpoint_path: æ£€æŸ¥ç‚¹è·¯å¾„
        device: è®¾å¤‡
        
    Returns:
        (æ¨¡å‹, é…ç½®)
    """
    logger.info(f"æ­£åœ¨åŠ è½½æ£€æŸ¥ç‚¹: {checkpoint_path}")
    
    # åŠ è½½æ£€æŸ¥ç‚¹
    if os.path.isdir(checkpoint_path):
        model_path = os.path.join(checkpoint_path, "pytorch_model.bin")
        config_path = os.path.join(checkpoint_path, "config.json")
    else:
        model_path = checkpoint_path
        config_path = None
    
    checkpoint = torch.load(model_path, map_location=device)
    
    # åŠ è½½é…ç½®
    if config_path and os.path.exists(config_path):
        with open(config_path, 'r') as f:
            config = json.load(f)
    else:
        # ä½¿ç”¨é»˜è®¤é…ç½®
        config = {
            'vocab_size': 50257,
            'hidden_size': 512,
            'num_layers': 6,
            'num_heads': 8,
            'max_seq_len': 1024,
            'dropout': 0.1
        }
        logger.warning("æœªæ‰¾åˆ°é…ç½®æ–‡ä»¶ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
    
    # åˆ›å»ºæ¨¡å‹
    model = GPTSmall(
        vocab_size=config['vocab_size'],
        hidden_size=config['hidden_size'],
        num_layers=config['num_layers'],
        num_heads=config['num_heads'],
        max_seq_len=config['max_seq_len'],
        dropout=config['dropout']
    )
    
    # åŠ è½½æƒé‡
    if 'model_state_dict' in checkpoint:
        model.load_state_dict(checkpoint['model_state_dict'])
    else:
        model.load_state_dict(checkpoint)
    
    model = model.to(device)
    model.eval()
    
    logger.info(f"æ¨¡å‹åŠ è½½å®Œæˆï¼Œå‚æ•°æ•°é‡: {model.get_num_params():,}")
    
    return model, config

def evaluate_perplexity_detailed(model: GPTSmall, val_loader, device: torch.device, tokenizer, show_examples: bool = True) -> float:
    """
    è®¡ç®—æ¨¡å‹åœ¨éªŒè¯é›†ä¸Šçš„å›°æƒ‘åº¦ï¼Œå¹¶æ˜¾ç¤ºè¯¦ç»†çš„è¯„ä¼°æ ·ä¾‹

    Args:
        model: æ¨¡å‹
        val_loader: éªŒè¯æ•°æ®åŠ è½½å™¨
        device: è®¾å¤‡
        tokenizer: åˆ†è¯å™¨
        show_examples: æ˜¯å¦æ˜¾ç¤ºè¯„ä¼°æ ·ä¾‹

    Returns:
        å›°æƒ‘åº¦
    """
    logger.info("æ­£åœ¨è®¡ç®—å›°æƒ‘åº¦...")

    model.eval()
    total_loss = 0.0
    total_tokens = 0
    batch_losses = []

    with torch.no_grad():
        for i, batch in enumerate(val_loader):
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)

            # æ„é€ è¾“å…¥å’Œç›®æ ‡
            inputs = input_ids[:, :-1]
            targets = input_ids[:, 1:]

            # å‰å‘ä¼ æ’­
            logits = model(inputs, attention_mask[:, :-1])

            # è®¡ç®—æŸå¤±
            loss = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id, reduction='sum')(
                logits.reshape(-1, logits.size(-1)),
                targets.reshape(-1)
            )

            # è®¡ç®—æœ‰æ•ˆtokenæ•°é‡
            valid_tokens = (targets != tokenizer.pad_token_id).sum().item()

            if valid_tokens > 0:
                batch_loss = loss.item() / valid_tokens
                batch_losses.append(batch_loss)
                total_loss += loss.item()
                total_tokens += valid_tokens

                # æ˜¾ç¤ºè¯¦ç»†è¯„ä¼°æ ·ä¾‹
                if show_examples and i < 3:  # æ˜¾ç¤ºå‰3ä¸ªæ‰¹æ¬¡çš„è¯¦ç»†ä¿¡æ¯
                    logger.info(f"\n=== è¯„ä¼°æ ·ä¾‹ {i+1} ===")

                    # é€‰æ‹©ç¬¬ä¸€ä¸ªæ ·æœ¬è¿›è¡Œè¯¦ç»†åˆ†æ
                    sample_input = inputs[0]
                    sample_target = targets[0]
                    sample_logits = logits[0]

                    # è§£ç åŸå§‹æ–‡æœ¬
                    original_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)
                    input_text = tokenizer.decode(sample_input, skip_special_tokens=True)
                    target_text = tokenizer.decode(sample_target, skip_special_tokens=True)

                    logger.info(f"åŸå§‹æ–‡æœ¬: {original_text[:200]}...")
                    logger.info(f"è¾“å…¥æ–‡æœ¬: {input_text[:150]}...")
                    logger.info(f"ç›®æ ‡æ–‡æœ¬: {target_text[:150]}...")

                    # è®¡ç®—è¯¥æ ·æœ¬çš„å›°æƒ‘åº¦
                    sample_loss = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)(
                        sample_logits.reshape(-1, sample_logits.size(-1)),
                        sample_target.reshape(-1)
                    )
                    sample_ppl = torch.exp(sample_loss).item()
                    logger.info(f"æ ·æœ¬æŸå¤±: {sample_loss.item():.4f}")
                    logger.info(f"æ ·æœ¬å›°æƒ‘åº¦: {sample_ppl:.2f}")

                    # åˆ†æå‰å‡ ä¸ªé¢„æµ‹
                    logger.info("å‰10ä¸ªtokené¢„æµ‹åˆ†æ:")
                    for j in range(min(10, len(sample_target))):
                        if sample_target[j] != tokenizer.pad_token_id:
                            # è·å–é¢„æµ‹æ¦‚ç‡
                            token_logits = sample_logits[j]
                            token_probs = torch.softmax(token_logits, dim=-1)

                            # çœŸå®token
                            true_token_id = sample_target[j].item()
                            true_token = tokenizer.decode([true_token_id])
                            true_prob = token_probs[true_token_id].item()

                            # é¢„æµ‹çš„æœ€é«˜æ¦‚ç‡token
                            pred_token_id = torch.argmax(token_probs).item()
                            pred_token = tokenizer.decode([pred_token_id])
                            pred_prob = token_probs[pred_token_id].item()

                            correct = "âœ“" if pred_token_id == true_token_id else "âœ—"

                            logger.info(f"  ä½ç½®{j:2d}: çœŸå®='{true_token}' (æ¦‚ç‡={true_prob:.3f}) | "
                                      f"é¢„æµ‹='{pred_token}' (æ¦‚ç‡={pred_prob:.3f}) {correct}")

            if (i + 1) % 50 == 0:
                logger.info(f"å·²å¤„ç† {i + 1} ä¸ªæ‰¹æ¬¡...")

    avg_loss = total_loss / total_tokens
    perplexity = torch.exp(torch.tensor(avg_loss)).item()

    # è®¡ç®—å›°æƒ‘åº¦ç»Ÿè®¡ä¿¡æ¯
    if batch_losses:
        import numpy as np
        batch_ppls = [math.exp(loss) for loss in batch_losses]
        logger.info(f"\n=== å›°æƒ‘åº¦ç»Ÿè®¡ ===")
        logger.info(f"å¹³å‡å›°æƒ‘åº¦: {perplexity:.2f}")
        logger.info(f"å›°æƒ‘åº¦æ ‡å‡†å·®: {np.std(batch_ppls):.2f}")
        logger.info(f"å›°æƒ‘åº¦èŒƒå›´: {min(batch_ppls):.2f} - {max(batch_ppls):.2f}")

    logger.info(f"å›°æƒ‘åº¦è®¡ç®—å®Œæˆ: {perplexity:.2f}")

    return perplexity

def evaluate_perplexity(model: GPTSmall, val_loader, device: torch.device, tokenizer) -> float:
    """
    è®¡ç®—æ¨¡å‹åœ¨éªŒè¯é›†ä¸Šçš„å›°æƒ‘åº¦ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
    """
    return evaluate_perplexity_detailed(model, val_loader, device, tokenizer, show_examples=False)

def evaluate_perplexity_on_prompts(model: GPTSmall, prompts: list, device: torch.device, tokenizer) -> float:
    """
    è®¡ç®—æ¨¡å‹åœ¨ç»™å®šæç¤ºè¯ä¸Šçš„å›°æƒ‘åº¦
    
    Args:
        model: æ¨¡å‹
        prompts: æç¤ºè¯åˆ—è¡¨
        device: è®¾å¤‡
        tokenizer: åˆ†è¯å™¨
        
    Returns:
        å›°æƒ‘åº¦
    """
    logger.info("æ­£åœ¨è®¡ç®—ç»™å®šæç¤ºè¯çš„å›°æƒ‘åº¦...")
    
    model.eval()
    total_loss = 0.0
    total_tokens = 0
    
    with torch.no_grad():
        for prompt in prompts:
            # ç¼–ç æç¤ºè¯
            input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)
            attention_mask = torch.ones(input_ids.shape, device=device)  # å‡è®¾æ²¡æœ‰paddingï¼Œæ‰€æœ‰ä½ç½®éƒ½æ˜¯æœ‰æ•ˆçš„
            
            # æ„é€ è¾“å…¥å’Œç›®æ ‡
            inputs = input_ids[:, :-1]
            targets = input_ids[:, 1:]
            
            # å‰å‘ä¼ æ’­
            logits = model(inputs, attention_mask[:, :-1])
            
            # è®¡ç®—æŸå¤±
            loss = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id, reduction='sum')(
                logits.reshape(-1, logits.size(-1)),
                targets.reshape(-1)
            )
            
            # è®¡ç®—æœ‰æ•ˆtokenæ•°é‡
            valid_tokens = (targets != tokenizer.pad_token_id).sum().item()
            
            total_loss += loss.item()
            total_tokens += valid_tokens
            
            logger.info(f"å¤„ç†æç¤ºè¯: '{prompt}'")
            logger.info(f"æŸå¤±ï¼š'{loss.item()}'")
            perplexity = torch.exp(loss)
            logger.info(f"å›°æƒ‘åº¦ï¼š'{perplexity.item()}'")

    
    avg_loss = total_loss / total_tokens
    perplexity = torch.exp(torch.tensor(avg_loss)).item()
    
    logger.info(f"å›°æƒ‘åº¦è®¡ç®—å®Œæˆ: {perplexity:.2f}")
    
    return perplexity


def evaluate_text_quality(text: str, prompt: str) -> dict:
    """
    è¯„ä¼°ç”Ÿæˆæ–‡æœ¬çš„è´¨é‡

    Args:
        text: ç”Ÿæˆçš„æ–‡æœ¬
        prompt: åŸå§‹æç¤ºè¯

    Returns:
        è¯„ä¼°ç»“æœå­—å…¸
    """
    # åŸºæœ¬ç»Ÿè®¡
    total_length = len(text)
    prompt_length = len(prompt)
    generated_length = total_length - prompt_length

    # è¯æ±‡ç»Ÿè®¡
    words = text.split()
    unique_words = len(set(words))
    word_diversity = unique_words / len(words) if words else 0

    # é‡å¤æ£€æµ‹
    sentences = text.split('.')
    repeated_phrases = 0
    for i, sentence in enumerate(sentences):
        for j, other_sentence in enumerate(sentences):
            if i != j and sentence.strip() and sentence.strip() in other_sentence:
                repeated_phrases += 1
                break

    # è¿è´¯æ€§è¯„åˆ†ï¼ˆç®€å•å¯å‘å¼ï¼‰
    coherence_score = 0
    if "once upon a time" in text.lower():
        coherence_score += 1
    if any(word in text.lower() for word in ["the", "and", "was", "were", "is", "are"]):
        coherence_score += 1
    if len(sentences) > 1:
        coherence_score += 1

    # è¯­æ³•å®Œæ•´æ€§ï¼ˆç®€å•æ£€æŸ¥ï¼‰
    grammar_score = 0
    if text.endswith('.') or text.endswith('!') or text.endswith('?'):
        grammar_score += 1
    if text[0].isupper():
        grammar_score += 1

    return {
        'total_length': total_length,
        'generated_length': generated_length,
        'word_count': len(words),
        'unique_words': unique_words,
        'word_diversity': word_diversity,
        'repeated_phrases': repeated_phrases,
        'coherence_score': coherence_score,
        'grammar_score': grammar_score,
        'sentences': len(sentences)
    }

def generate_text_samples_with_scoring(model: GPTSmall, tokenizer: GPT2Tokenizer, device: torch.device, num_samples: int = 5):
    """
    ç”Ÿæˆæ–‡æœ¬æ ·æœ¬å¹¶è¿›è¡Œè¯¦ç»†è¯„åˆ†

    Args:
        model: æ¨¡å‹
        tokenizer: åˆ†è¯å™¨
        device: è®¾å¤‡
        num_samples: ç”Ÿæˆæ ·æœ¬æ•°é‡
    """
    logger.info(f"æ­£åœ¨ç”Ÿæˆ {num_samples} ä¸ªæ–‡æœ¬æ ·æœ¬å¹¶è¯„åˆ†...")

    model.eval()

    # é¢„å®šä¹‰çš„æç¤ºè¯
    prompts = [
        "Once upon a time",
        "The little girl",
        "In a magical forest",
        "The brave knight",
        "A beautiful princess",
        "Tom likes drinking juice",
        "I love you",
        "Why training a natural language model is important",
        "If I were a god"
    ]

    all_scores = []

    for i in range(min(num_samples, len(prompts))):
        prompt = prompts[i]
        logger.info(f"\n{'='*60}")
        logger.info(f"æ ·æœ¬ {i+1}: '{prompt}'")
        logger.info(f"{'='*60}")

        # ç¼–ç æç¤ºè¯
        input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)

        # è®¡ç®—æç¤ºè¯çš„å›°æƒ‘åº¦
        with torch.no_grad():
            prompt_logits = model(input_ids)
            if input_ids.size(1) > 1:
                prompt_targets = input_ids[:, 1:]
                prompt_inputs = input_ids[:, :-1]
                prompt_loss = nn.CrossEntropyLoss()(
                    prompt_logits[:, :-1, :].reshape(-1, prompt_logits.size(-1)),
                    prompt_targets.reshape(-1)
                )
                prompt_ppl = torch.exp(prompt_loss).item()
            else:
                prompt_ppl = float('inf')

        logger.info(f"æç¤ºè¯å›°æƒ‘åº¦: {prompt_ppl:.2f}")

        # ç”Ÿæˆå¤šä¸ªç‰ˆæœ¬è¿›è¡Œæ¯”è¾ƒ
        temperatures = [0.7, 0.8, 1.0]
        best_text = ""
        best_score = -1

        for temp in temperatures:
            logger.info(f"\n--- æ¸©åº¦ {temp} ---")

            # ç”Ÿæˆæ–‡æœ¬
            with torch.no_grad():
                generated = model.generate(
                    input_ids,
                    max_new_tokens=80,
                    temperature=temp,
                    top_k=50
                )

            # è§£ç ç”Ÿæˆçš„æ–‡æœ¬
            generated_text = tokenizer.decode(generated[0], skip_special_tokens=True)
            logger.info(f"ç”Ÿæˆæ–‡æœ¬: {generated_text}")

            # è¯„ä¼°æ–‡æœ¬è´¨é‡
            quality_scores = evaluate_text_quality(generated_text, prompt)

            # è®¡ç®—ç”Ÿæˆéƒ¨åˆ†çš„å›°æƒ‘åº¦
            if generated.size(1) > input_ids.size(1):
                generated_part = generated[:, input_ids.size(1):]
                with torch.no_grad():
                    gen_logits = model(generated[:, :-1])
                    gen_targets = generated[:, 1:]
                    gen_loss = nn.CrossEntropyLoss()(
                        gen_logits[:, input_ids.size(1)-1:, :].reshape(-1, gen_logits.size(-1)),
                        gen_targets[:, input_ids.size(1)-1:].reshape(-1)
                    )
                    gen_ppl = torch.exp(gen_loss).item()
            else:
                gen_ppl = float('inf')

            # ç»¼åˆè¯„åˆ†
            total_score = (
                quality_scores['coherence_score'] * 2 +
                quality_scores['grammar_score'] * 2 +
                quality_scores['word_diversity'] * 3 +
                max(0, 3 - quality_scores['repeated_phrases']) +
                max(0, 5 - gen_ppl/10)  # å›°æƒ‘åº¦è¶Šä½è¶Šå¥½
            )

            logger.info(f"è´¨é‡è¯„åˆ†:")
            logger.info(f"  ç”Ÿæˆé•¿åº¦: {quality_scores['generated_length']} å­—ç¬¦")
            logger.info(f"  è¯æ±‡æ•°é‡: {quality_scores['word_count']}")
            logger.info(f"  è¯æ±‡å¤šæ ·æ€§: {quality_scores['word_diversity']:.3f}")
            logger.info(f"  è¿è´¯æ€§è¯„åˆ†: {quality_scores['coherence_score']}/3")
            logger.info(f"  è¯­æ³•è¯„åˆ†: {quality_scores['grammar_score']}/2")
            logger.info(f"  é‡å¤çŸ­è¯­: {quality_scores['repeated_phrases']}")
            logger.info(f"  ç”Ÿæˆå›°æƒ‘åº¦: {gen_ppl:.2f}")
            logger.info(f"  ç»¼åˆè¯„åˆ†: {total_score:.2f}")

            if total_score > best_score:
                best_score = total_score
                best_text = generated_text
                best_temp = temp

        logger.info(f"\nğŸ† æœ€ä½³ç”Ÿæˆ (æ¸©åº¦={best_temp}, è¯„åˆ†={best_score:.2f}):")
        logger.info(f"'{best_text}'")

        all_scores.append({
            'prompt': prompt,
            'best_text': best_text,
            'best_score': best_score,
            'best_temperature': best_temp
        })

    # è¾“å‡ºæ€»ä½“ç»Ÿè®¡
    logger.info(f"\n{'='*60}")
    logger.info("æ€»ä½“è¯„ä¼°ç»Ÿè®¡:")
    logger.info(f"{'='*60}")

    avg_score = sum(s['best_score'] for s in all_scores) / len(all_scores)
    logger.info(f"å¹³å‡è¯„åˆ†: {avg_score:.2f}")

    best_sample = max(all_scores, key=lambda x: x['best_score'])
    worst_sample = min(all_scores, key=lambda x: x['best_score'])

    logger.info(f"æœ€ä½³æ ·æœ¬: '{best_sample['prompt']}' (è¯„åˆ†: {best_sample['best_score']:.2f})")
    logger.info(f"æœ€å·®æ ·æœ¬: '{worst_sample['prompt']}' (è¯„åˆ†: {worst_sample['best_score']:.2f})")

def generate_text_samples(model: GPTSmall, tokenizer: GPT2Tokenizer, device: torch.device, num_samples: int = 5):
    """
    ç”Ÿæˆæ–‡æœ¬æ ·æœ¬ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
    """
    generate_text_samples_with_scoring(model, tokenizer, device, num_samples)

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='æ¨¡å‹è¯„ä¼°')
    parser.add_argument('--checkpoint_path', type=str, required=True, help='æ¨¡å‹æ£€æŸ¥ç‚¹è·¯å¾„')
    parser.add_argument('--batch_size', type=int, default=8, help='æ‰¹é‡å¤§å°')
    parser.add_argument('--max_eval_batches', type=int, default=100, help='æœ€å¤§è¯„ä¼°æ‰¹æ¬¡æ•°')
    parser.add_argument('--generate_samples', action='store_true', help='æ˜¯å¦ç”Ÿæˆæ–‡æœ¬æ ·æœ¬')
    parser.add_argument('--num_samples', type=int, default=5, help='ç”Ÿæˆæ ·æœ¬æ•°é‡')
    
    args = parser.parse_args()
    
    # è®¾ç½®è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    logger.info(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åŠ è½½æ¨¡å‹
    model, config = load_model_from_checkpoint(args.checkpoint_path, device)
    
    # åˆ›å»ºåˆ†è¯å™¨
    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    # åˆ›å»ºéªŒè¯æ•°æ®åŠ è½½å™¨
    logger.info("æ­£åœ¨åˆ›å»ºéªŒè¯æ•°æ®åŠ è½½å™¨...")
    _, val_loader, _ = create_dataloaders(
        batch_size=args.batch_size,
        max_length=config['max_seq_len'],
        num_workers=2
    )

    # é™åˆ¶è¯„ä¼°æ‰¹æ¬¡æ•°
    if args.max_eval_batches > 0:
        val_data = []
        for i, batch in enumerate(val_loader):
            if i >= args.max_eval_batches:
                break
            val_data.append(batch)
        val_loader = val_data
        logger.info(f"é™åˆ¶è¯„ä¼°æ‰¹æ¬¡æ•°ä¸º: {len(val_loader)}")

    # è®¡ç®—éªŒè¯é›†å›°æƒ‘åº¦ï¼ˆè¯¦ç»†ç‰ˆæœ¬ï¼‰
    logger.info("\n" + "="*60)
    logger.info("å¼€å§‹è¯¦ç»†è¯„ä¼°éªŒè¯é›†å›°æƒ‘åº¦")
    logger.info("="*60)
    perplexity = evaluate_perplexity_detailed(model, val_loader, device, tokenizer, show_examples=True)

    # ç”Ÿæˆæ–‡æœ¬æ ·æœ¬
    if args.generate_samples:
        generate_text_samples(model, tokenizer, device, args.num_samples)
    
    # è¾“å‡ºç»“æœæ‘˜è¦
    logger.info("\n=== è¯„ä¼°ç»“æœæ‘˜è¦ ===")
    logger.info(f"æ¨¡å‹æ£€æŸ¥ç‚¹: {args.checkpoint_path}")
    logger.info(f"æ¨¡å‹å‚æ•°æ•°é‡: {model.get_num_params():,}")
    logger.info(f"éªŒè¯é›†å›°æƒ‘åº¦: {perplexity:.2f}")
    
    if perplexity < 40:
        logger.info("âœ“ å›°æƒ‘åº¦è¾¾åˆ°ç›®æ ‡è¦æ±‚ (< 40)")
    else:
        logger.info("âœ— å›°æƒ‘åº¦æœªè¾¾åˆ°ç›®æ ‡è¦æ±‚ (< 40)")

if __name__ == "__main__":
    main()

"""
æ•°æ®éªŒè¯è„šæœ¬
éªŒè¯è®­ç»ƒæ•°æ®çš„tokenåŒ–æ˜¯å¦æ­£ç¡®ï¼Œæ£€æŸ¥æ•°æ®å®Œæ•´æ€§
"""

import sys
import os
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(str(Path(__file__).parent.parent))

import torch
from data.tinystories_loader import create_dataloaders, verify_data_integrity
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def detailed_token_analysis(train_loader, val_loader, tokenizer, num_samples: int = 5):
    """
    è¯¦ç»†åˆ†ætokenåŒ–è¿‡ç¨‹
    
    Args:
        train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
        val_loader: éªŒè¯æ•°æ®åŠ è½½å™¨
        tokenizer: åˆ†è¯å™¨
        num_samples: åˆ†æçš„æ ·æœ¬æ•°é‡
    """
    logger.info("=== è¯¦ç»†Tokenåˆ†æ ===")
    
    # åˆ†æè®­ç»ƒé›†
    logger.info("\n--- è®­ç»ƒé›†åˆ†æ ---")
    train_iter = iter(train_loader)
    
    for i in range(min(num_samples, len(train_loader))):
        batch = next(train_iter)
        logger.info(f"\nè®­ç»ƒæ ·æœ¬ {i+1}:")
        
        # è·å–ç¬¬ä¸€ä¸ªæ ·æœ¬
        input_ids = batch['input_ids'][0]
        attention_mask = batch['attention_mask'][0]
        
        logger.info(f"Tokenåºåˆ—å½¢çŠ¶: {input_ids.shape}")
        logger.info(f"æ³¨æ„åŠ›æ©ç å½¢çŠ¶: {attention_mask.shape}")
        
        # ç»Ÿè®¡ä¿¡æ¯
        total_tokens = len(input_ids)
        valid_tokens = attention_mask.sum().item()
        pad_tokens = total_tokens - valid_tokens
        
        logger.info(f"æ€»tokenæ•°: {total_tokens}")
        logger.info(f"æœ‰æ•ˆtokenæ•°: {valid_tokens}")
        logger.info(f"å¡«å……tokenæ•°: {pad_tokens}")
        
        # è§£ç å®Œæ•´æ–‡æœ¬
        full_text = tokenizer.decode(input_ids, skip_special_tokens=True)
        logger.info(f"å®Œæ•´æ–‡æœ¬é•¿åº¦: {len(full_text)} å­—ç¬¦")
        logger.info(f"å®Œæ•´æ–‡æœ¬: {full_text}")
        
        # åˆ†æå‰20ä¸ªtoken
        logger.info("å‰20ä¸ªtokenè¯¦ç»†åˆ†æ:")
        for j in range(min(20, len(input_ids))):
            token_id = input_ids[j].item()
            token_text = tokenizer.decode([token_id])
            is_special = token_id in [tokenizer.pad_token_id, tokenizer.eos_token_id, tokenizer.bos_token_id]
            mask_value = attention_mask[j].item()
            
            logger.info(f"  ä½ç½®{j:2d}: ID={token_id:5d} | æ–‡æœ¬='{token_text}' | "
                       f"ç‰¹æ®Š={is_special} | æ©ç ={mask_value}")
        
        # éªŒè¯è¯­è¨€å»ºæ¨¡çš„è¾“å…¥-ç›®æ ‡å¯¹åº”å…³ç³»
        logger.info("è¯­è¨€å»ºæ¨¡è¾“å…¥-ç›®æ ‡éªŒè¯:")
        inputs = input_ids[:-1]  # å‰n-1ä¸ªtokenä½œä¸ºè¾“å…¥
        targets = input_ids[1:]  # ån-1ä¸ªtokenä½œä¸ºç›®æ ‡
        
        logger.info(f"è¾“å…¥åºåˆ—é•¿åº¦: {len(inputs)}")
        logger.info(f"ç›®æ ‡åºåˆ—é•¿åº¦: {len(targets)}")
        
        # æ£€æŸ¥å‰10ä¸ªè¾“å…¥-ç›®æ ‡å¯¹
        for j in range(min(10, len(inputs))):
            input_token = tokenizer.decode([inputs[j].item()])
            target_token = tokenizer.decode([targets[j].item()])
            logger.info(f"  è¾“å…¥'{input_token}' -> ç›®æ ‡'{target_token}'")
    
    # åˆ†æéªŒè¯é›†
    logger.info("\n--- éªŒè¯é›†åˆ†æ ---")
    val_iter = iter(val_loader)
    
    for i in range(min(2, len(val_loader))):  # éªŒè¯é›†åªçœ‹2ä¸ªæ ·æœ¬
        batch = next(val_iter)
        logger.info(f"\néªŒè¯æ ·æœ¬ {i+1}:")
        
        input_ids = batch['input_ids'][0]
        attention_mask = batch['attention_mask'][0]
        
        # åŸºæœ¬ç»Ÿè®¡
        total_tokens = len(input_ids)
        valid_tokens = attention_mask.sum().item()
        
        logger.info(f"æ€»tokenæ•°: {total_tokens}")
        logger.info(f"æœ‰æ•ˆtokenæ•°: {valid_tokens}")
        
        # è§£ç æ–‡æœ¬
        full_text = tokenizer.decode(input_ids, skip_special_tokens=True)
        logger.info(f"æ–‡æœ¬: {full_text[:200]}...")

def check_data_consistency(train_loader, val_loader, tokenizer):
    """
    æ£€æŸ¥æ•°æ®ä¸€è‡´æ€§
    
    Args:
        train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
        val_loader: éªŒè¯æ•°æ®åŠ è½½å™¨
        tokenizer: åˆ†è¯å™¨
    """
    logger.info("=== æ•°æ®ä¸€è‡´æ€§æ£€æŸ¥ ===")
    
    # æ£€æŸ¥è¯è¡¨èŒƒå›´
    logger.info("æ£€æŸ¥token IDèŒƒå›´...")
    
    all_token_ids = set()
    max_token_id = 0
    min_token_id = float('inf')
    
    # æ£€æŸ¥è®­ç»ƒé›†
    for i, batch in enumerate(train_loader):
        if i >= 10:  # åªæ£€æŸ¥å‰10ä¸ªæ‰¹æ¬¡
            break
        
        input_ids = batch['input_ids']
        batch_max = input_ids.max().item()
        batch_min = input_ids.min().item()
        
        max_token_id = max(max_token_id, batch_max)
        min_token_id = min(min_token_id, batch_min)
        
        # æ”¶é›†æ‰€æœ‰token ID
        all_token_ids.update(input_ids.flatten().tolist())
    
    logger.info(f"Token IDèŒƒå›´: {min_token_id} - {max_token_id}")
    logger.info(f"è¯è¡¨å¤§å°: {len(tokenizer)}")
    logger.info(f"å”¯ä¸€tokenæ•°é‡: {len(all_token_ids)}")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰è¶…å‡ºè¯è¡¨çš„token
    if max_token_id >= len(tokenizer):
        logger.error(f"å‘ç°è¶…å‡ºè¯è¡¨èŒƒå›´çš„token: {max_token_id} >= {len(tokenizer)}")
    else:
        logger.info("âœ“ æ‰€æœ‰tokenéƒ½åœ¨è¯è¡¨èŒƒå›´å†…")
    
    # æ£€æŸ¥ç‰¹æ®Štoken
    logger.info("ç‰¹æ®Štokenæ£€æŸ¥:")
    logger.info(f"  PAD token: {tokenizer.pad_token} (ID: {tokenizer.pad_token_id})")
    logger.info(f"  EOS token: {tokenizer.eos_token} (ID: {tokenizer.eos_token_id})")
    logger.info(f"  BOS token: {getattr(tokenizer, 'bos_token', 'None')} (ID: {getattr(tokenizer, 'bos_token_id', 'None')})")
    
    # ç»Ÿè®¡ç‰¹æ®Štokenä½¿ç”¨æƒ…å†µ
    pad_count = sum(1 for tid in all_token_ids if tid == tokenizer.pad_token_id)
    eos_count = sum(1 for tid in all_token_ids if tid == tokenizer.eos_token_id)
    
    logger.info(f"  PAD tokenå‡ºç°æ¬¡æ•°: {pad_count}")
    logger.info(f"  EOS tokenå‡ºç°æ¬¡æ•°: {eos_count}")

def verify_batch_shapes(train_loader, val_loader):
    """
    éªŒè¯æ‰¹æ¬¡å½¢çŠ¶ä¸€è‡´æ€§
    
    Args:
        train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
        val_loader: éªŒè¯æ•°æ®åŠ è½½å™¨
    """
    logger.info("=== æ‰¹æ¬¡å½¢çŠ¶éªŒè¯ ===")
    
    # æ£€æŸ¥è®­ç»ƒé›†æ‰¹æ¬¡
    logger.info("è®­ç»ƒé›†æ‰¹æ¬¡å½¢çŠ¶:")
    for i, batch in enumerate(train_loader):
        if i >= 3:  # åªæ£€æŸ¥å‰3ä¸ªæ‰¹æ¬¡
            break
        
        input_ids = batch['input_ids']
        attention_mask = batch['attention_mask']
        
        logger.info(f"  æ‰¹æ¬¡ {i+1}: input_ids={input_ids.shape}, attention_mask={attention_mask.shape}")
        
        # æ£€æŸ¥å½¢çŠ¶ä¸€è‡´æ€§
        if input_ids.shape != attention_mask.shape:
            logger.error(f"å½¢çŠ¶ä¸ä¸€è‡´: input_ids={input_ids.shape} vs attention_mask={attention_mask.shape}")
        else:
            logger.info(f"    âœ“ å½¢çŠ¶ä¸€è‡´")
    
    # æ£€æŸ¥éªŒè¯é›†æ‰¹æ¬¡
    logger.info("éªŒè¯é›†æ‰¹æ¬¡å½¢çŠ¶:")
    for i, batch in enumerate(val_loader):
        if i >= 3:  # åªæ£€æŸ¥å‰3ä¸ªæ‰¹æ¬¡
            break
        
        input_ids = batch['input_ids']
        attention_mask = batch['attention_mask']
        
        logger.info(f"  æ‰¹æ¬¡ {i+1}: input_ids={input_ids.shape}, attention_mask={attention_mask.shape}")

def main():
    """ä¸»å‡½æ•°"""
    logger.info("å¼€å§‹æ•°æ®éªŒè¯...")
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨ï¼ˆä½¿ç”¨å°è§„æ¨¡æ•°æ®è¿›è¡ŒéªŒè¯ï¼‰
    logger.info("åˆ›å»ºæ•°æ®åŠ è½½å™¨...")
    train_loader, val_loader, tokenizer = create_dataloaders(
        batch_size=4,
        max_length=512,
        num_workers=0,  # é¿å…å¤šè¿›ç¨‹é—®é¢˜
        train_subset_size=20,  # ä½¿ç”¨å°è§„æ¨¡æ•°æ®
        val_subset_size=10
    )
    
    logger.info(f"è®­ç»ƒæ‰¹æ¬¡æ•°: {len(train_loader)}")
    logger.info(f"éªŒè¯æ‰¹æ¬¡æ•°: {len(val_loader)}")
    logger.info(f"åˆ†è¯å™¨è¯è¡¨å¤§å°: {len(tokenizer)}")
    
    # æ‰§è¡Œå„ç§éªŒè¯
    try:
        # 1. åŸºæœ¬æ•°æ®å®Œæ•´æ€§éªŒè¯
        verify_data_integrity(train_loader, val_loader, tokenizer, num_samples=3)
        
        # 2. è¯¦ç»†tokenåˆ†æ
        detailed_token_analysis(train_loader, val_loader, tokenizer, num_samples=3)
        
        # 3. æ•°æ®ä¸€è‡´æ€§æ£€æŸ¥
        check_data_consistency(train_loader, val_loader, tokenizer)
        
        # 4. æ‰¹æ¬¡å½¢çŠ¶éªŒè¯
        verify_batch_shapes(train_loader, val_loader)
        
        logger.info("\n" + "="*60)
        logger.info("ğŸ‰ æ‰€æœ‰æ•°æ®éªŒè¯é€šè¿‡ï¼")
        logger.info("æ•°æ®å·²å‡†å¤‡å°±ç»ªï¼Œå¯ä»¥å¼€å§‹è®­ç»ƒã€‚")
        logger.info("="*60)
        
    except Exception as e:
        logger.error(f"\nâŒ æ•°æ®éªŒè¯å¤±è´¥: {e}")
        logger.error("è¯·æ£€æŸ¥æ•°æ®å¤„ç†æµç¨‹ã€‚")
        raise

if __name__ == "__main__":
    main()
